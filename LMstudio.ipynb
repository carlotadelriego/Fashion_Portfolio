{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wit import Wit  # Importamos la librería de Wit.ai\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "from transformers import pipeline  # Para GPT-2\n",
    "import torch  # Para visión por computadora\n",
    "from torchvision import models, transforms  # Para procesamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_data = [\n",
    "'/Users/carlotafernandez/Desktop/Code/FASHION/Fashion_Portfolio-1/Mode_system.pdf',\n",
    "'/Users/carlotafernandez/Desktop/Code/FASHION/Fashion_Portfolio-1/christianDior.pdf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping\n",
    "urls = [\n",
    "    'https://www.vogue.com/article/9-outdated-fashion-pieces-vogue-editors-still-love', \n",
    "    'https://www.vogue.com/article/metropolitan-museum-lorna-simpson', \n",
    "    'https://www.elle.com/culture/music/a63324386/lady-gaga-mayhem-interview-2025/'\n",
    "]\n",
    "\n",
    "# Función para hacer web scraping y extraer los textos de los enlaces\n",
    "def web_scraping(urls):\n",
    "    texts = []\n",
    "    for link in urls:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "scraped_texts = web_scraping(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para hacer web scraping y extraer los textos de los enlaces\n",
    "def web_scraping(urls):\n",
    "    texts = []\n",
    "    for link in urls:\n",
    "        response = requests.get(link)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "scraped_texts = web_scraping(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PREPROCESAMIENTO DEL TEXTO ######\n",
    "# Limpieza del texto\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 1. TOKENIZATION - SPACY\n",
    "documents = files_data + scraped_texts\n",
    "tokenized_documents = [nlp(document) for document in documents]\n",
    "\n",
    "# 2. ELIMINAR STOPWORDS\n",
    "def remove_stopwords(tokenized_docs):\n",
    "    filtered_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        filtered_tokens = [token.text for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "        filtered_docs.append(\" \".join(filtered_tokens))  # Convertir a string\n",
    "    return filtered_docs\n",
    "\n",
    "\n",
    "filtered_documents = remove_stopwords(tokenized_documents)\n",
    "\n",
    "# 3. NORMALIZACIÓN (LEMATIZACIÓN)\n",
    "def normalize_tokens(tokenized_docs):\n",
    "    normalized_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        normalized_tokens = [token.lemma_.lower() for token in doc if token.lemma_!= '-PRON-' and token.is_alpha]\n",
    "        normalized_docs.append(\" \".join(normalized_tokens))  # Convertir a string\n",
    "    return normalized_docs\n",
    "\n",
    "\n",
    "normalized_documents = normalize_tokens(tokenized_documents)\n",
    "\n",
    "# 4. PROCESAR DOCUMENTOS PARA TF-IDF\n",
    "processed_documents = filtered_documents + normalized_documents  # Debe ser una lista de strings\n",
    "\n",
    "# 5. EXTRACCIÓN DE PALABRAS CLAVE CON TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=0.1)\n",
    "X = vectorizer.fit_transform(processed_documents)\n",
    "\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X.toarray().mean(axis=0)  # Promedio de los scores por documento\n",
    "df_keywords = pd.DataFrame({'keyword': keywords, 'score': tfidf_scores})\n",
    "\n",
    "fashion_keywords = {\"fashion\", \"style\", \"dress\", \"clothing\", \"trend\", \"outfit\", \"wear\", \"designer\", \"runway\", \"chic\", \"elegant\"}\n",
    "df_keywords = df_keywords[df_keywords['keyword'].isin(fashion_keywords)]\n",
    "df_keywords = df_keywords.sort_values(by='score', ascending=False)\n",
    "print(df_keywords.head(10))\n",
    "\n",
    "# 6. APLICAR EMBEDDINGS PARA ENTENDER EL SIGNIFICADO\n",
    "def get_similar_fashion_terms(word, n=5):\n",
    "    \"\"\" Encuentra palabras similares a un término de moda usando embeddings de spaCy \"\"\"\n",
    "    token = nlp.vocab[word]\n",
    "    similar_words = sorted(nlp.vocab, key=lambda w: token.similarity(w), reverse=True)\n",
    "    \n",
    "    # Filtrar palabras con sentido y devolver las más cercanas\n",
    "    return [w.text for w in similar_words if w.is_alpha and w.has_vector][:n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A DATAFRAME WITH THE PROCESSED TEXT\n",
    "df = pd.DataFrame({\n",
    "    'Original': [' '.join([token.text for token in doc]) for doc in tokenized_documents],\n",
    "    'Filtered': [' '.join(doc) for doc in filtered_documents],\n",
    "    'Normalized': [' '.join(doc) for doc in normalized_documents]\n",
    "})\n",
    "# SAVE RESULTS ON A CSV\n",
    "df.to_csv('preprocessed_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### INTEGRACIÓN DE WIT.AI ######\n",
    "# Token de acceso de Wit.ai\n",
    "WIT_AI_TOKEN = 'FG2VHKVK6SXU5NSGYKQ65LQZETS5ROQH'\n",
    "\n",
    "# Inicializar el cliente de Wit.ai\n",
    "client = Wit(WIT_AI_TOKEN)\n",
    "\n",
    "# Función para procesar el mensaje del usuario con Wit.ai\n",
    "def get_message_wit(message):\n",
    "    resp = client.message(message)\n",
    "    intent = resp['intents'][0]['name'] if resp['intents'] else None\n",
    "    entities = resp['entities']\n",
    "    return intent, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### GENERAR DESCRIPCIONES AUTOMÁTICAS DE OUTFITS ######\n",
    "# Cargar modelo de visión por computadora (ResNet)\n",
    "vision_model = models.resnet50(weights='IMAGENET1K_V1')  # Carga las pesas de imagenet.\n",
    "vision_model.eval()\n",
    "\n",
    "# Cargar modelo de generación de texto (GPT-2)\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\", framework=\"pt\")\n",
    "\n",
    "# Función para generar descripciones basadas en imágenes\n",
    "def generate_description_from_image(image_path):\n",
    "    # Preprocesar la imagen\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    # Obtener características de la imagen\n",
    "    with torch.no_grad():\n",
    "        features = vision_model(image)\n",
    "    \n",
    "    # Generar descripción usando GPT-2\n",
    "    description = text_generator(\"This outfit features\", max_length=50, num_return_sequences=1)\n",
    "    return description[0]['generated_text']\n",
    "\n",
    "\n",
    "# Función para generar descripciones basadas en preferencias\n",
    "def generate_description_from_preferences(preferences):\n",
    "    color = preferences.get('color', 'neutral')\n",
    "    style = preferences.get('style', 'casual')\n",
    "    occasion = preferences.get('occasion', 'everyday')\n",
    "    \n",
    "    prompt = f\"A {color} {style} outfit suitable for {occasion} occasions. This outfit includes\"\n",
    "    description = text_generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    return description[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### INTERFAZ GRÁFICA ######\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from PIL import Image, ImageTk\n",
    "from rasa.core.agent import Agent\n",
    "import os\n",
    "\n",
    "# Cargar el modelo entrenado de Rasa\n",
    "agent = Agent.load(\"models\")\n",
    "\n",
    "# Función para procesar el mensaje y obtener la respuesta de Rasa\n",
    "def get_message_rasa(message):\n",
    "    response = agent.handle_text(message)\n",
    "    return response[0]['text']\n",
    "\n",
    "def chatbot_interface():\n",
    "    # Crear una ventana\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Fashion Assistant Chatbot\")\n",
    "\n",
    "    # Cargar y mostrar la imagen\n",
    "    image = Image.open('/path/to/your/logo.jpg') \n",
    "    image = image.resize((200, 150), Image.Resampling.LANCZOS)  # Redimensionar la imagen\n",
    "    img = ImageTk.PhotoImage(image)\n",
    "\n",
    "    # Crear un widget Label para mostrar la imagen\n",
    "    image_label = tk.Label(window, image=img)\n",
    "    image_label.grid(row=0, column=1, padx=10, pady=10)  # Posicionar la imagen al lado\n",
    "\n",
    "    # Crear un área de texto con scroll para mostrar la conversación\n",
    "    conversation_area = scrolledtext.ScrolledText(window, width=90, height=40, wrap=tk.WORD, state=tk.DISABLED)\n",
    "    conversation_area.grid(row=0, column=0, padx=10, pady=10)\n",
    "\n",
    "    # Crear una caja de entrada para que el usuario escriba su mensaje\n",
    "    user_input_box = tk.Entry(window, width=60)\n",
    "    user_input_box.grid(row=1, column=0, padx=10, pady=10)\n",
    "\n",
    "    # Función para manejar el envío de mensajes\n",
    "    def send_message():\n",
    "        user_input = user_input_box.get()\n",
    "        if user_input.strip():  # Evitar mensajes vacíos\n",
    "            conversation_area.config(state=tk.NORMAL)\n",
    "            conversation_area.insert(tk.END, f\"You: {user_input}\\n\")  # Mostrar la entrada del usuario\n",
    "            conversation_area.yview(tk.END)\n",
    "            \n",
    "            # Obtener la respuesta del chatbot usando Rasa\n",
    "            rasa_response = get_message_rasa(user_input)\n",
    "            \n",
    "            conversation_area.insert(tk.END, f\"Chatbot: {rasa_response}\\n\")  # Mostrar la respuesta del chatbot\n",
    "            conversation_area.yview(tk.END)\n",
    "            \n",
    "            user_input_box.delete(0, tk.END)  # Limpiar la caja de entrada\n",
    "\n",
    "    # Crear un botón para enviar el mensaje\n",
    "    send_button = tk.Button(window, text=\"Send\", width=10, command=send_message)\n",
    "    send_button.grid(row=1, column=1, padx=10, pady=10)\n",
    "\n",
    "    # Ejecutar el bucle principal de la GUI\n",
    "    window.mainloop()\n",
    "\n",
    "# Ejecutar la interfaz del chatbot\n",
    "chatbot_interface()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
